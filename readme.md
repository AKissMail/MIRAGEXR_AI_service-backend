# MirageXR AI Services

This server backend implements a facade to AI services, providing a wrapper to [MirageXR](https://github.com/WEKIT-ECS/MIRAGE-XR), 
specifically to facilitate dialogue interaction with Large Language Models (LLMs) using Speech-To-Text (STT) and Text-To-Speech 
(TTS) services. LLM interaction follows the Retrieval-Augmented Generation (RAG) pattern, see [Lewis et al.](https://arxiv.org/abs/2005.11401),
and manages a data pipeline for storing documents as corpora, enhancing the dialogue experience in eXtended Reality settings.

## Table of Contents

- [Introduction](#introduction)
- [Endpoints and Features](#features)
- [License](#license)

## Introduction

The MirageXR AI service backend extends the capabilities of the MirageXR project by adding a wrapper to external AI services 
for advanced dialogue functionalities. It serves as a facade to different LLMs, STT, and TTS models, providing endpoints 
that incorporate RAG patterns for dynamic conversation flow. The backend also includes a data pipeline to manage and store 
various documents within a corpus, making it a versatile tool for dialogue management in XR environments.

## Features

### Endpoints:

- **`admin/`**: Django Admin UI
  - **Note**: Should be deactivated before the server is deployed to ensure security.
- **`speak/`**: Text-to-Speech Conversion
  - Converts provided text into speech. Supports various configurations for voice and language.
- **`listen/`**: Audio to Text Transcription
  - Accepts audio files in MP3, WAV, and OGG formats. Returns a transcript using Whisper models.
- **`think/`**: Large Language Model (LLM) Operations
  - Supports three different LLM models: GPT-3.5, GPT-4, and the RAG Model. Can perform a variety of NLP tasks.
- **`options/`**: Model Options Inquiry
  - Offers a JSON response detailing the available models.
- **`authentication/`**: API Authentication
  - Provides an API token for authenticated access. Requires a valid user account.

- A set of Postman request configurations is available for testing the API endpoints. These can be found in the `./postman_test_request` directory of the project repository.


### Data Pipeline:

- Processes and stores content from PDF, TXT, and CSV files in a database, making it accessible for RAG. 
- All documents in /data/ will be processed. With the command `data`, the data will be uploaded  and processed to the database and will be embedded and analyzed.

### # Project Databases Configuration

In the project, two databases are utilized:

1. **Django's Default Database**: This database is used to store information from documents. It leverages Django's out-of-the-box database management capabilities to handle data persistence and retrieval efficiently.

2. **Chroma Framework Database**: For managing embeddings, an additional database generated by the Chroma framework is used. This specialized database is designed to optimize the storage and querying of embedding vectors, enhancing the project's performance in handling complex data operations.



Django DB 
![DjangoDB.png](readMe/DjangoDB.png)
ChromaDB
![chromaDB.png](readMe/chromaDB.png)

### Set up
1. Clone the project.
2. If not done install pip
3. Run python -m venv venv in the project directory
4. Activate the Virtual Environment -- On Windows: .\venv\Scripts\activate -- On macOS or Linux: source venv/bin/activate
5. Install with pip all the dependencies: `pip install -r requirements.txt`
6. Set up an environment as shown in section `.env`
7. Download the classifiers model from fastText and place it in the root directory. Either download cc.no.300.bin model 
or update the think/apps.py with you prefer model.
8. `cd backend`
9. Run the Django server with `python manage.py runserver 8000 &`.
10. Set up the database with `python manage.py makemigrations think` and `python manage.py makemigrations document`
11. and `python manage.py migrate`.
12. Create a superuser with `python manage.py createsuperuser`.
13. Access the backend via `http://127.0.0.1:8000/admin/`, and create a new user (optional) and an authentication token, to be used with the API request.
14. You can test the System with `python manage.py test`. 
15. You can upload document for a RAG Model via the `/document/` endpoint and set up the Configuration use the `/document/configuration` endpoint.
16. Don't forget CORS_ORIGIN_WHITELIST in the settings.py or suffer like me for 1 hour. 
17. To send your request (e.g., via Postman) and include the key in the header with the line key `Authorization` set to `Token $key`.

### .env

`OPENAI_API_KEY=$your_key`

`GOOGLE_API_KEY=$your_key`

`GOOGLE_TTS_ENDPOINT=https://texttospeech.googleapis.com/v1/text:synthesize`

`VECTOR_DB=$path to vectorDB`

### Configuration of a RAG model configuration
The configuration of an RAG model is done via JSON file in the config/think folder. Each JSON file provides some settings 
and the possibility to create a prompt around the User input. The need parameter are listed below together with the legal 
values. The name of the file name must be "apiName" + ".json". 

` {
  "provider": ["openai"],
  "model": ["gpt-4", "gpt-4-turbo-preview", "gpt-3.5-turbo"],
  "rag_function": ["jaccard", "chromadb", "faiss"],
  "rag_function_call": ["jaccard_index", "vector_chromadb", "query_faiss_embeddings"],
  "apiName": "Can be chosen freely, but must be unique",
  "prompt_start": "This string is at the beginning of the prompt",
  "prompt_end": "This string is at the end of the prompt",
  "context_start": "This string is at the beginning of the context",
  "context_end": "This string is at the end of the context"
}
`

### Todos prior to the deployment
1. Update manage.py
   1. Update the SECRET_KEY - For development purposes, a SECRET_KEY is in the manage.py. This key has to be changed!
   2. Disable the DEBUG variable.
   3. Update the list of ALLOWED_HOSTS and remove the IPs that are used for development.
   4. Set the CORS_ORIGIN_ALLOW_ALL to false and set this up properly.

## License

This project is licensed under the MIT License - see the [LICENSE](readMe/License.md) file for details.
